{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#DECISION TREE"
      ],
      "metadata": {
        "id": "fIA1LfwcuQpK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1: What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        "ans-\n",
        "\n",
        "    A Decision Tree is a supervised machine-learning model that splits data into branches based on feature values, forming a tree-like structure.\n",
        "    classification:\n",
        "\n",
        "    The tree starts at the root node.\n",
        "\n",
        "    It chooses the best feature and best threshold that separates classes.\n",
        "\n",
        "    It creates decision nodes until the data becomes pure or stopping criteria are met.\n",
        "\n",
        "    A leaf node assigns the final class label.\n",
        "\n",
        "\n",
        "2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "ans-\n",
        "\n",
        "    Gini Impurity\n",
        "  \n",
        "    Formula:\n",
        "         \n",
        "         G=1−i=1∑k​pi2\n",
        "\t​\n",
        "    Measures how often a randomly chosen element would be incorrectly labelled.\n",
        "\n",
        "    Entropy\n",
        "\n",
        "    Formula:\n",
        "\n",
        "          H=−i=1∑kpilog2(pi)\n",
        "\n",
        "   \n",
        "    Measures impurity based on information theory.\n",
        "\n",
        "    Impact on Splits\n",
        "\n",
        "    Both choose the purest split.\n",
        "\n",
        "    Gini is faster and prefers the most frequent class.\n",
        "\n",
        "    Entropy is more sensitive to all class distributions.\n",
        "\n",
        "    Both aim to reduce impurity, improving classification accuracy.\n",
        "\n",
        "3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each\n",
        "\n",
        "ans-\n",
        "\n",
        "    Pre-Pruning (Early Stopping)\n",
        "\n",
        "    Stops tree growth early using conditions (e.g., max_depth, min_samples_split).\n",
        "    Advantage: Prevents overfitting early → faster training.\n",
        "\n",
        "    Post-Pruning\n",
        "\n",
        "    Grow full tree first then prune weak branches using validation data.\n",
        "    Advantage: Produces simpler and more accurate models.\n",
        "\n",
        "4: What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "ans-\n",
        "\n",
        "    Information Gain = Reduction in impurity after a split.\n",
        "\n",
        "     IG=Impurity(parent)−∑Impurity(children)\n",
        "\n",
        "     It is important for choosing the best spilt-\n",
        "\n",
        "    Helps select the best feature to split.\n",
        "\n",
        "    More Information Gain = better separation between classes.\n",
        "\n",
        "    Drives optimal tree structure.\n",
        "\n",
        "5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "ans-\n",
        "\n",
        "  Some common real-world applications of decisions trees are-\n",
        "    \n",
        "    -Medical diagnosis\n",
        "\n",
        "    -Fraud detection\n",
        "\n",
        "    -Credit risk assessment\n",
        "\n",
        "    -Customer segmentation\n",
        "\n",
        "    -Manufacturing defect prediction\n",
        "\n",
        "  Their main advantages are-\n",
        "    \n",
        "    -Easy to interpret\n",
        "\n",
        "    -Works with both numerical & categorical data\n",
        "\n",
        "    -Requires little data preprocessing  \n",
        "\n",
        "  The limitations are-\n",
        "\n",
        "    -Prone to overfitting\n",
        "\n",
        "    -Unstable with small data changes\n",
        "\n",
        "    -Not good with continuous boundaries  \n",
        "   \n",
        "\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-3PmG2yHuUU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances"
      ],
      "metadata": {
        "id": "N3RMjsvxv2qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train using Gini\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\")\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Feature Importances:\", clf.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXUMLdJhv4Zf",
        "outputId": "7352f2ee-1b34-4d00-fb0f-c7b1442527c1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature Importances: [0.01911002 0.         0.89326355 0.08762643]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree"
      ],
      "metadata": {
        "id": "a-6MafkbwEfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Full Tree\n",
        "full_tree = DecisionTreeClassifier()\n",
        "full_tree.fit(X_train, y_train)\n",
        "full_acc = accuracy_score(y_test, full_tree.predict(X_test))\n",
        "\n",
        "# Depth 3 Tree\n",
        "small_tree = DecisionTreeClassifier(max_depth=3)\n",
        "small_tree.fit(X_train, y_train)\n",
        "small_acc = accuracy_score(y_test, small_tree.predict(X_test))\n",
        "\n",
        "print(\"Full Tree Accuracy:\", full_acc)\n",
        "print(\"Max Depth=3 Accuracy:\", small_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRo5cdGEwHqY",
        "outputId": "b6f38290-9511-48a3-a9e8-441ddad8c8cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Tree Accuracy: 1.0\n",
            "Max Depth=3 Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 8: Write a Python program to:\n",
        "● Load the Boston Housing Dataset\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "Hn7Nv6fOwWNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Model\n",
        "reg = DecisionTreeRegressor()\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "# MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"Feature Importances:\", reg.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz2n3-3NwZ90",
        "outputId": "eacd23c8-1185-455d-cc24-a5b9855d4a9c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.5341680816352551\n",
            "Feature Importances: [0.52439682 0.05208106 0.04843452 0.02755707 0.03172974 0.13854318\n",
            " 0.08903023 0.08822739]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#since boston data has been deleted from scikit hence clifornia housing is taken\n"
      ],
      "metadata": {
        "id": "t4BAICEixbpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "8iDyS-layDLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NznKl2hFyJmf",
        "outputId": "f5d066b5-f76e-490e-e166-4933f9781e20"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n"
      ],
      "metadata": {
        "id": "j52psDcEySCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ans-\n",
        "\n",
        "1.Handling Missing Values\n",
        "\n",
        "    -Use mean/median for numerical features.\n",
        "\n",
        "    -Use most frequent category for categorical columns.\n",
        "\n",
        "     Alternatively use sklearn's SimpleImputer.\n",
        "\n",
        "2.Encoding Categorical Features\n",
        "\n",
        "Options:\n",
        "\n",
        "    One-Hot Encoding (preferred for tree models)\n",
        "\n",
        "    Label Encoding (when categories have natural order)\n",
        "\n",
        "3.Training Decision Tree\n",
        "\n",
        "  Steps:\n",
        "\n",
        "    Split dataset into train/test.\n",
        "\n",
        "    Build a DecisionTreeClassifier.\n",
        "\n",
        "    Fit model on training data.\n",
        "\n",
        "4.Hyperparameter Tuning\n",
        "\n",
        "    Use GridSearchCV for:\n",
        "\n",
        "    max_depth\n",
        "\n",
        "    min_samples_split\n",
        "\n",
        "    min_samples_leaf\n",
        "\n",
        "    criterion (gini/entropy)\n",
        "\n",
        "5.Model Evaluation\n",
        "\n",
        "    Metrics:\n",
        "\n",
        "    Accuracy\n",
        "\n",
        "    Precision, Recall, F1-score\n",
        "\n",
        "    Confusion Matrix\n",
        "\n",
        "6.Business Value\n",
        "\n",
        "    Early identification of disease risks\n",
        "\n",
        "    Reduce manual workload for doctors\n",
        "\n",
        "    Faster diagnosis\n",
        "\n",
        "    Personalized patient monitoring\n",
        "\n",
        "    Cost reduction by optimizing interventions\n",
        "\n",
        "    This model helps healthcare providers predict diseases accurately, improving patient outcomes."
      ],
      "metadata": {
        "id": "iewSJt9lydPF"
      }
    }
  ]
}